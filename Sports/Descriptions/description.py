import pandas as pd
import numpy as np
import os
import re
import string
from datetime import datetime
from textblob import TextBlob
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import joblib

# ä¸‹è½½NLTKèµ„æº
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')  # æ·»åŠ POSæ ‡æ³¨å™¨
nltk.download('omw-1.4')  # æ·»åŠ Open Multilingual WordNet
nltk.download('punkt_tab')


# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = "description_analysis"
os.makedirs(output_dir, exist_ok=True)

# åŠ è½½æ•°æ®
df = pd.read_csv('sports_2024-2025_with_popularity.csv')

# 1. è§†é¢‘æ—¶é•¿åˆ†ç±»
def classify_duration(duration):
    if duration <= 240:  # 4åˆ†é’Ÿ=240ç§’
        return 'short'
    elif 240 < duration <= 1200:  # 20åˆ†é’Ÿ=1200ç§’
        return 'medium'
    else:
        return 'long'

df['duration_type'] = df['duration_seconds'].apply(classify_duration)

# 2. æ—¶é—´è¡°å‡æƒé‡è®¡ç®—
df['published_at'] = pd.to_datetime(df['published_at'])
latest_date = df['published_at'].max()
df['days_since_publish'] = (latest_date - df['published_at']).dt.days

def plateau_decay(t, early_a=0.25, t0=20, floor=0.35):
    decay = 1 / (1 + np.exp(early_a * (t - t0)))
    return np.maximum(decay, floor)

df['time_weight'] = plateau_decay(df['days_since_publish'])
df['adj_popularity'] = df['popularity_normalized'] * 100000 * df['time_weight']

# 3. ç®€ä»‹æ–‡æœ¬é¢„å¤„ç†
def clean_text(text):
    if pd.isna(text):
        return ""
    
    # è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    
    # ç§»é™¤URL
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<.*?>', '', text)
    
    # ç§»é™¤æ ‡ç‚¹ç¬¦å· - ä¿ç•™å¥ç‚¹ç”¨äºå¥å­åˆ†å‰²
    text = text.translate(str.maketrans('', '', string.punctuation.replace('.', '')))
    
    # ç§»é™¤æ•°å­—
    text = re.sub(r'\d+', '', text)
    
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# æƒ…æ„Ÿåˆ†æ
def get_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

# æ–‡æœ¬ç»Ÿè®¡ç‰¹å¾
def get_text_features(text):
    # ä½¿ç”¨ç©ºæ ¼åˆ†è¯æ›¿ä»£nltk.word_tokenize
    words = text.split()
    
    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡ç‰¹å¾
    word_count = len(words)
    avg_word_length = np.mean([len(word) for word in words]) if words else 0
    unique_word_ratio = len(set(words)) / word_count if word_count else 0
    
    # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
    sentiment = TextBlob(text).sentiment.polarity
    
    return {
        'word_count': word_count,
        'avg_word_length': avg_word_length,
        'unique_word_ratio': unique_word_ratio,
        'sentiment': sentiment
    }

# åº”ç”¨é¢„å¤„ç† - ç›´æ¥ä½¿ç”¨get_text_featuresåŒ…å«æƒ…æ„Ÿåˆ†æ
df['cleaned_description'] = df['description'].apply(clean_text)


# æå–æ–‡æœ¬ç»Ÿè®¡ç‰¹å¾
text_features = df['cleaned_description'].apply(get_text_features).apply(pd.Series)
df = pd.concat([df, text_features], axis=1)

# ä¿å­˜å¤„ç†åçš„æ•°æ®
processed_csv_path = os.path.join(output_dir, "processed_data_with_description.csv")
df.to_csv(processed_csv_path, index=False, encoding='utf-8-sig')
print(f"âœ… å·²ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°: {processed_csv_path}")

# 4. æ–‡æœ¬å‘é‡åŒ–å’Œä¸»é¢˜å»ºæ¨¡
# å‡†å¤‡åœç”¨è¯
stop_words = set(stopwords.words('english'))
custom_stopwords = ['subscribe', 'channel', 'follow', 'like', 'instagram', 'facebook', 
                   'twitter', 'tiktok', 'website', 'click', 'link', 'watch', 'video']
stop_words.update(custom_stopwords)

# è¯å½¢è¿˜åŸ
lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    words = word_tokenize(text)
    return " ".join([lemmatizer.lemmatize(word) for word in words])

df['lemmatized_description'] = df['cleaned_description'].apply(lemmatize_text)

# TF-IDFå‘é‡åŒ–
tfidf_vectorizer = TfidfVectorizer(
    max_features=1000,
    stop_words=list(stop_words),
    ngram_range=(1, 2)
)

tfidf_matrix = tfidf_vectorizer.fit_transform(df['lemmatized_description'])

# ä¸»é¢˜å»ºæ¨¡ (LDA)
num_topics = 5
lda = LatentDirichletAllocation(
    n_components=num_topics,
    max_iter=10,
    learning_method='online',
    random_state=42
)

lda_topics = lda.fit_transform(tfidf_matrix)

# æ·»åŠ ä¸»é¢˜åˆ°æ•°æ®æ¡†
for i in range(num_topics):
    df[f'topic_{i}'] = lda_topics[:, i]

# å¯è§†åŒ–ä¸»é¢˜å…³é”®è¯
def plot_top_words(model, feature_names, n_top_words, title):
    fig, axes = plt.subplots(1, num_topics, figsize=(20, 6), sharex=True)
    axes = axes.flatten()
    
    for topic_idx, topic in enumerate(model.components_):
        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
        top_features = [feature_names[i] for i in top_features_ind]
        weights = topic[top_features_ind]

        ax = axes[topic_idx]
        ax.barh(top_features, weights, height=0.7)
        ax.set_title(f'Topic {topic_idx + 1}', fontdict={'fontsize': 14})
        ax.invert_yaxis()
        ax.tick_params(axis='both', which='major', labelsize=12)
        
        for i in 'top right left'.split():
            ax.spines[i].set_visible(False)
    
    plt.suptitle(title, fontsize=16)
    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)
    return fig

# è·å–ç‰¹å¾åç§°
tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()

# ç»˜åˆ¶ä¸»é¢˜å…³é”®è¯
topic_plot = plot_top_words(lda, tfidf_feature_names, 10, "ä¸»é¢˜å…³é”®è¯")
topic_plot_path = os.path.join(output_dir, "topic_keywords.png")
plt.savefig(topic_plot_path, dpi=300, bbox_inches='tight')
plt.close()
print(f"âœ… å·²ä¿å­˜ä¸»é¢˜å…³é”®è¯å›¾åƒåˆ°: {topic_plot_path}")




# 5. åˆ†ç»„å»ºæ¨¡ä¸è¯„ä¼°
results = {}
models = {}
keyword_importances = {}

# æ›´æ–°æ–‡æœ¬ç‰¹å¾åˆ—è¡¨ - ä½¿ç”¨å®é™…æå–çš„ç‰¹å¾
text_features = [
    'word_count', 'avg_word_length', 'unique_word_ratio', 'sentiment'
] + [f'topic_{i}' for i in range(num_topics)]

# ç¡®ä¿è¿™äº›ç‰¹å¾å­˜åœ¨äºDataFrameä¸­
missing_features = [f for f in text_features if f not in df.columns]
if missing_features:
    print(f"âš ï¸ è­¦å‘Š: ä»¥ä¸‹ç‰¹å¾ç¼ºå¤±: {missing_features}")
    text_features = [f for f in text_features if f in df.columns]

print("å»ºæ¨¡ä½¿ç”¨çš„ç‰¹å¾:", text_features)  # è°ƒè¯•è¾“å‡º

# å¯è§†åŒ–è®¾ç½®
sns.set_style("whitegrid")
plt.rcParams['font.size'] = 12

scalers = {}  # æ·»åŠ è¿™ä¸ªå­—å…¸ä¿å­˜æ ‡å‡†åŒ–å™¨
feature_names_by_duration = {}  # æ·»åŠ è¿™ä¸ªå­—å…¸ä¿å­˜ç‰¹å¾åç§°

for dtype in ['short', 'medium', 'long']:
    print(f"\n===== Modeling {dtype} videos =====")
    subset = df[df['duration_type'] == dtype]
    
    # æ£€æŸ¥æ ·æœ¬é‡
    if len(subset) < 20:
        print(f"âš ï¸ è­¦å‘Š: {dtype}ç±»å‹è§†é¢‘åªæœ‰{len(subset)}ä¸ªæ ·æœ¬ï¼Œè·³è¿‡å»ºæ¨¡")
        continue
    
    # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡ - ç¡®ä¿åªä½¿ç”¨å­˜åœ¨çš„åˆ—
    available_features = [f for f in text_features if f in subset.columns]
    X = subset[available_features]
    y = subset['adj_popularity']
    
    # ä¿å­˜ç‰¹å¾åç§°
    feature_names_by_duration[dtype] = available_features

    # æ•°æ®é›†åˆ’åˆ†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ä¿å­˜æ ‡å‡†åŒ–å™¨
    scalers[dtype] = scaler

    # å»ºæ¨¡
    model = RandomForestRegressor(
        n_estimators=200,
        max_depth=10,
        min_samples_split=5,
        random_state=42
    )
    model.fit(X_train_scaled, y_train)
    
    # ä¿å­˜æ¨¡å‹
    model_path = os.path.join(output_dir, f"{dtype}_description_model.joblib")
    joblib.dump(model, model_path)
    print(f"âœ… å·²ä¿å­˜æ¨¡å‹åˆ°: {model_path}")
    
    
    # é¢„æµ‹ä¸è¯„ä¼°
    y_pred = model.predict(X_test_scaled)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    
    # äº¤å‰éªŒè¯
    if len(X_train) >= 5:
        cv_scores = cross_val_score(
            model, X_train_scaled, y_train, 
            cv=min(5, len(X_train)), 
            scoring='r2'
        )
        cv_mean = np.mean(cv_scores)
        cv_std = np.std(cv_scores)
    else:
        cv_mean = np.nan
        cv_std = np.nan
    
    print(f"RMSE: {rmse:.4f}")
    print(f"RÂ²: {r2:.4f}")
    print(f"äº¤å‰éªŒè¯RÂ²: {cv_mean:.4f} Â± {cv_std:.4f}")
    
    # ä¿å­˜ç»“æœ
    results[dtype] = {
        'rmse': rmse,
        'r2': r2,
        'cv_r2': cv_mean,
        'cv_std': cv_std,
        'num_samples': len(subset)
    }
    models[dtype] = model

    # ç‰¹å¾é‡è¦æ€§åˆ†æ
    feat_importances = pd.Series(
        model.feature_importances_,
        index=X.columns
    ).sort_values(ascending=False)
    
    # ä¿å­˜ç‰¹å¾é‡è¦æ€§
    keyword_importances[dtype] = feat_importances
    importance_csv_path = os.path.join(output_dir, f"{dtype}_feature_importance.csv")
    feat_importances.to_csv(importance_csv_path, header=['importance'])
    print(f"âœ… å·²ä¿å­˜ç‰¹å¾é‡è¦æ€§æ•°æ®åˆ°: {importance_csv_path}")
    
    # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
    plt.figure(figsize=(12, 8))
    top_features = feat_importances.nlargest(10)
    ax = top_features.sort_values().plot.barh(color='teal')
    plt.title(f"{dtype.capitalize()}è§†é¢‘ - ç‰¹å¾é‡è¦æ€§", fontsize=16)
    plt.xlabel("ç‰¹å¾é‡è¦æ€§", fontsize=12)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, v in enumerate(top_features.sort_values()):
        ax.text(v + 0.001, i, f"{v:.4f}", color='black', va='center')
    
    # ä¿å­˜å›¾åƒ
    importance_img_path = os.path.join(output_dir, f"{dtype}_feature_importance.png")
    plt.savefig(importance_img_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… å·²ä¿å­˜ç‰¹å¾é‡è¦æ€§å›¾åƒåˆ°: {importance_img_path}")
    
    # åˆ›å»ºé¢„æµ‹ä¸å®é™…å€¼å¯¹æ¯”å›¾
    plt.figure(figsize=(10, 6))
    plt.scatter(y_test, y_pred, alpha=0.6, color='darkorange')
    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)
    plt.title(f"{dtype.capitalize()}è§†é¢‘ - é¢„æµ‹å€¼ vs å®é™…å€¼", fontsize=14)
    plt.xlabel("å®é™…æµè¡Œåº¦", fontsize=12)
    plt.ylabel("é¢„æµ‹æµè¡Œåº¦", fontsize=12)
    
    # æ·»åŠ RÂ²æ–‡æœ¬
    plt.text(0.05, 0.9, f"RÂ² = {r2:.3f}", transform=plt.gca().transAxes, fontsize=12)
    
    # ä¿å­˜å›¾åƒ
    scatter_img_path = os.path.join(output_dir, f"{dtype}_prediction_scatter.png")
    plt.savefig(scatter_img_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ… å·²ä¿å­˜é¢„æµ‹å¯¹æ¯”å›¾åƒåˆ°: {scatter_img_path}")

# 6. AIè¾…åŠ©æ’°å†™æ¡†æ¶
class DescriptionAssistant:
    def __init__(self, models, vectorizer, lda, text_features, scalers=None, feature_names_by_duration=None, topic_names=None):
        self.models = models
        self.scalers = scalers or {} # æ·»åŠ æ ‡å‡†åŒ–å™¨å­—å…¸
        self.vectorizer = vectorizer
        self.lda = lda
        self.text_features = text_features
        self.feature_names_by_duration = feature_names_by_duration or {}
        self.topic_names = topic_names or {
            0: "è®¢é˜…ä¸å…³æ³¨",
            1: "èµ›äº‹ä¿¡æ¯",
            2: "é€‰æ‰‹äº®ç‚¹",
            3: "èƒŒæ™¯æ•…äº‹",
            4: "æƒ…æ„Ÿå…±é¸£"
        }
    
    # ä¿®æ”¹ analyze_description æ–¹æ³•
    def analyze_description(self, description, duration_type):
        """åˆ†æç°æœ‰ç®€ä»‹çš„è´¨é‡ï¼ˆä¿®å¤ç‰¹å¾ç»´åº¦é”™è¯¯å’Œæ ‡å‡†åŒ–å…¼å®¹æ€§ï¼‰"""
        if duration_type not in self.models:
            return {"error": f"æ²¡æœ‰{duration_type}ç±»å‹è§†é¢‘çš„æ¨¡å‹"}
        if duration_type not in self.scalers:
            return {"error": f"æ²¡æœ‰{duration_type}ç±»å‹è§†é¢‘çš„æ ‡å‡†åŒ–å™¨"}
        if duration_type not in self.feature_names_by_duration:
            return {"error": f"æ²¡æœ‰{duration_type}ç±»å‹è§†é¢‘çš„ç‰¹å¾åç§°ä¿¡æ¯"}

        # è·å–æ¨¡å‹ã€æ ‡å‡†åŒ–å™¨å’ŒæœŸæœ›ç‰¹å¾å
        model = self.models[duration_type]
        scaler = self.scalers[duration_type]
        expected_features = self.feature_names_by_duration[duration_type]

        # æ–‡æœ¬é¢„å¤„ç†
        cleaned_text = clean_text(description)
        lemmatized_text = lemmatize_text(cleaned_text)

        # æå–æ–‡æœ¬ç»Ÿè®¡ç‰¹å¾ + æƒ…æ„Ÿ
        features = get_text_features(cleaned_text)
        features['sentiment'] = get_sentiment(cleaned_text)

        # æå–ä¸»é¢˜åˆ†å¸ƒ
        tfidf_vec = self.vectorizer.transform([lemmatized_text])
        topic_dist = self.lda.transform(tfidf_vec)[0]
        num_topics = len(self.lda.components_)
        for i in range(num_topics):
            features[f'topic_{i}'] = topic_dist[i] if i < len(topic_dist) else 0

        # æ„é€ ç‰¹å¾å‘é‡ï¼Œç¡®ä¿é¡ºåºä¸€è‡´
        feature_list = []
        missing_features = []
        for feat in expected_features:
            if feat in features:
                feature_list.append(features[feat])
            else:
                feature_list.append(0)
                missing_features.append(feat)

        if missing_features:
            print(f"âš ï¸ ç¼ºå¤±ç‰¹å¾ï¼ˆå·²å¡«0ï¼‰: {missing_features}")
        if len(feature_list) != len(expected_features):
            return {"error": f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {len(expected_features)} ä¸ªï¼Œå®é™… {len(feature_list)} ä¸ª"}

        # ä½¿ç”¨ DataFrame ä¸”åˆ—åä¸è®­ç»ƒä¸€è‡´ï¼ˆé˜²æ­¢è­¦å‘Šï¼‰
        feature_df = pd.DataFrame([feature_list], columns=expected_features)
        try:
            features_scaled = scaler.transform(feature_df)
        except Exception as e:
            return {"error": f"æ ‡å‡†åŒ–å¤±è´¥: {str(e)}"}

        # æ¨¡å‹é¢„æµ‹
        popularity_score = model.predict(features_scaled)[0]
        main_topic = np.argmax(topic_dist)
        topic_name = self.topic_names.get(main_topic, f"ä¸»é¢˜{main_topic}")
        topic_strength = topic_dist[main_topic]

        return {
            "popularity_score": popularity_score,
            "main_topic": topic_name,
            "topic_strength": topic_strength,
            "word_count": features.get('word_count', 0),
            "sentiment": features.get('sentiment', 0),
            "topic_distribution": topic_dist.tolist()
        }


        
    def generate_improvement_tips(self, analysis_result, duration_type):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        tips = []
        
        # æ ¹æ®è§†é¢‘ç±»å‹æä¾›å»ºè®®
        if duration_type == 'short':
            if analysis_result['word_count'] > 50:
                tips.append("ç®€ä»‹è¿‡é•¿ï¼šçŸ­è§†é¢‘ç®€ä»‹åº”ç®€æ´ï¼Œå»ºè®®æ§åˆ¶åœ¨50å­—ä»¥å†…")
            elif analysis_result['word_count'] < 20:
                tips.append("ç®€ä»‹è¿‡çŸ­ï¼šå¯æ·»åŠ æ›´å¤šå…³é”®è¯æå‡å¯æœç´¢æ€§")
        elif duration_type == 'medium':
            if analysis_result['word_count'] < 50:
                tips.append("ç®€ä»‹è¿‡çŸ­ï¼šä¸­è§†é¢‘å¯åŒ…å«æ›´å¤šç»†èŠ‚ï¼Œå»ºè®®50-150å­—")
        else:  # long
            if analysis_result['word_count'] < 100:
                tips.append("ç®€ä»‹è¿‡çŸ­ï¼šé•¿è§†é¢‘åº”æä¾›è¯¦ç»†èƒŒæ™¯ï¼Œå»ºè®®100å­—ä»¥ä¸Š")
        
        # æƒ…æ„Ÿå»ºè®®
        if analysis_result['sentiment'] < -0.3:
            tips.append("æƒ…æ„Ÿè¿‡äºè´Ÿé¢ï¼šä½“è‚²è§†é¢‘é€šå¸¸éœ€è¦ç§¯ææƒ…æ„Ÿï¼Œå»ºè®®è°ƒæ•´è¯­æ°”")
        elif analysis_result['sentiment'] > 0.7:
            tips.append("æƒ…æ„Ÿè¿‡äºå¤¸å¼ ï¼šå¯èƒ½é™ä½å¯ä¿¡åº¦ï¼Œå»ºè®®é€‚åº¦è°ƒæ•´")
        
        # ä¸»é¢˜å»ºè®®
        if duration_type == 'short' and analysis_result['main_topic'] == "èƒŒæ™¯æ•…äº‹":
            tips.append("ä¸»é¢˜ä¸åŒ¹é…ï¼šçŸ­è§†é¢‘æ›´é€‚åˆçªå‡ºé€‰æ‰‹äº®ç‚¹è€ŒéèƒŒæ™¯æ•…äº‹")
        
        if duration_type == 'long' and analysis_result['main_topic'] == "è®¢é˜…ä¸å…³æ³¨":
            tips.append("ä¸»é¢˜ä¸åŒ¹é…ï¼šé•¿è§†é¢‘åº”å‡å°‘å…³æ³¨å¼•å¯¼ï¼Œå¢åŠ æ·±åº¦å†…å®¹")
        
        # æ·»åŠ åŸºäºç‰¹å¾é‡è¦æ€§çš„å»ºè®®
        if duration_type in keyword_importances:
            top_features = keyword_importances[duration_type].nlargest(3).index.tolist()
            tips.append(f"é‡è¦ç‰¹å¾ï¼šå°è¯•ä¼˜åŒ– {', '.join(top_features)}")
        
        return tips
    
    def generate_description_template(self, duration_type, main_topic=None):
        """ç”Ÿæˆæè¿°æ¨¡æ¿"""
        templates = {
            'short': {
                'default': "ç²¾å½©æ—¶åˆ»ï¼š{highlight}ï¼ğŸ”¥ ä¸å®¹é”™è¿‡ï¼ #ä½“è‚² #ç²¾å½©ç¬é—´",
                'é€‰æ‰‹äº®ç‚¹': "æƒŠè‰³è¡¨ç°ï¼š{player} çš„ {skill}ï¼ğŸ‘ #ä½“è‚² #ç²¾å½©ç¬é—´",
                'æƒ…æ„Ÿå…±é¸£': "çƒ­è¡€æ²¸è…¾ï¼{event} çš„è¿™ä¸€åˆ»è®©äººéš¾å¿˜ â¤ï¸ #ä½“è‚² #æƒ…æ„Ÿ"
            },
            'medium': {
                'default': "èµ›äº‹é›†é”¦ï¼š{event} çš„ç²¾å½©å›é¡¾ã€‚åŒ…å« {key_moments} ç­‰å…³é”®æ—¶åˆ»ã€‚è®¢é˜…è·å–æ›´å¤šå†…å®¹ï¼",
                'èµ›äº‹ä¿¡æ¯': "{event} å®Œæ•´å›é¡¾ï¼šä» {start} åˆ° {end} çš„å…³é”®æ—¶åˆ»ã€‚ğŸ† #ä½“è‚² #èµ›äº‹",
                'èƒŒæ™¯æ•…äº‹': "èƒŒåçš„æ•…äº‹ï¼š{player} å¦‚ä½•å…‹æœå›°éš¾å‚åŠ  {event}ã€‚â¤ï¸ #ä½“è‚² #æ•…äº‹"
            },
            'long': {
                'default': "æ·±åº¦è§£æï¼š{event} çš„å®Œæ•´å›é¡¾ä¸åˆ†æã€‚ä»å†å²èƒŒæ™¯åˆ°æŠ€æœ¯ç»†èŠ‚ï¼Œå…¨é¢è§£è¯» {key_aspects}ã€‚",
                'èƒŒæ™¯æ•…äº‹': "ä¼ å¥‡ä¹‹è·¯ï¼š{player} çš„èŒä¸šç”Ÿæ¶¯å›é¡¾ä¸ {event} çš„å¹•åæ•…äº‹ã€‚ğŸ“– #ä½“è‚² #çºªå½•ç‰‡",
                'æŠ€æœ¯åˆ†æ': "æŠ€æœ¯æ‹†è§£ï¼šæ·±å…¥åˆ†æ {player} åœ¨ {event} ä¸­çš„ {technique} æŠ€æœ¯ã€‚ğŸ” #ä½“è‚² #åˆ†æ"
            }
        }
        
        if duration_type not in templates:
            return "æŠ±æ­‰ï¼Œæš‚ä¸æ”¯æŒæ­¤è§†é¢‘ç±»å‹çš„æ¨¡æ¿"
        
        if main_topic and main_topic in templates[duration_type]:
            return templates[duration_type][main_topic]
        else:
            return templates[duration_type]['default']

# åˆå§‹åŒ–AIåŠ©æ‰‹
topic_names = {
    0: "è®¢é˜…ä¸å…³æ³¨",
    1: "èµ›äº‹ä¿¡æ¯",
    2: "é€‰æ‰‹äº®ç‚¹",
    3: "èƒŒæ™¯æ•…äº‹",
    4: "æŠ€æœ¯åˆ†æ"
}

assistant = DescriptionAssistant(
    models=models,
    scalers=scalers,  # æ·»åŠ è¿™è¡Œ
    vectorizer=tfidf_vectorizer,
    lda=lda,
    text_features=text_features,
    feature_names_by_duration=feature_names_by_duration,  # æ·»åŠ è¿™è¡Œ
    topic_names=topic_names
)

# æµ‹è¯•AIåŠ©æ‰‹åŠŸèƒ½
test_descriptions = {
    'short': "Amazing goal by Messi! Watch now! #football #soccer",
    'medium': "Full highlights of the Lakers vs Warriors game. Key moments: LeBron's dunk, Curry's three-pointers. Subscribe for more NBA content.",
    'long': "In-depth analysis of the 2024 Australian Open. This documentary explores the journey of the top players, their training routines, and the key matches that defined the tournament."
}

assistant_results = {}
for dtype, desc in test_descriptions.items():
    analysis = assistant.analyze_description(desc, dtype)

    if "error" in analysis:
        print(f"âŒ {dtype} è§†é¢‘åˆ†æå¤±è´¥: {analysis['error']}")
        assistant_results[dtype] = {
            'analysis': analysis,
            'improvement_tips': [],
            'template': "æ— æ³•ç”Ÿæˆæ¨¡æ¿"
        }
        continue

    tips = assistant.generate_improvement_tips(analysis, dtype)
    template = assistant.generate_description_template(dtype, analysis['main_topic'])

    assistant_results[dtype] = {
        'analysis': analysis,
        'improvement_tips': tips,
        'template': template
    }

# å†™å…¥ AI åŠ©æ‰‹æµ‹è¯•ç»“æœ
assistant_path = os.path.join(output_dir, "ai_assistant_results.txt")
with open(assistant_path, 'w', encoding='utf-8') as f:
    f.write("AIåŠ©æ‰‹æµ‹è¯•ç»“æœ\n")
    f.write("="*50 + "\n\n")

    for dtype, result in assistant_results.items():
        f.write(f"è§†é¢‘ç±»å‹: {dtype}\n")
        f.write(f"åŸå§‹ç®€ä»‹: {test_descriptions[dtype]}\n")

        if "error" in result['analysis']:
            f.write(f"âŒ åˆ†æå¤±è´¥: {result['analysis']['error']}\n\n")
            f.write("-"*50 + "\n\n")
            continue

        f.write(f"é¢„æµ‹æµè¡Œåº¦: {result['analysis']['popularity_score']:.4f}\n")
        f.write(f"ä¸»è¦ä¸»é¢˜: {result['analysis']['main_topic']}\n")
        f.write(f"æƒ…æ„Ÿåˆ†æ•°: {result['analysis']['sentiment']:.2f}\n")
        f.write(f"å­—æ•°: {result['analysis']['word_count']}\n")

        f.write("\næ”¹è¿›å»ºè®®:\n")
        for tip in result['improvement_tips']:
            f.write(f" - {tip}\n")

        f.write(f"\næ¨èæ¨¡æ¿:\n{result['template']}\n\n")
        f.write("-"*50 + "\n\n")

print(f"âœ… å·²ä¿å­˜AIåŠ©æ‰‹æµ‹è¯•ç»“æœåˆ°: {assistant_path}")

# âœ… ä¿®å¤ scatterplot æŠ¥é”™å‰ï¼Œç¡®ä¿ word_count æ˜¯ä¸€ç»´æ•°å€¼ï¼š
if isinstance(df['word_count'].iloc[0], (list, np.ndarray)):
    df['word_count'] = df['word_count'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)

# è½¬æ¢ä¸ºæ•°å€¼ï¼Œé˜²æ­¢å¼‚å¸¸ç±»å‹æ®‹ç•™
for col in ['word_count', 'adj_popularity']:
    if col not in df.columns:
        print(f"âš ï¸ åˆ— {col} ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        continue

    value = df[col].iloc[0]

    print(f"ğŸ” æ£€æŸ¥ {col} ç¬¬ä¸€è¡Œå€¼ç±»å‹: {type(value)}")
    
    if isinstance(value, (list, np.ndarray)):
        print(f"ğŸ“› {col} æ˜¯åµŒå¥—æ•°ç»„ï¼Œå°è¯•å±•å¼€")
        df[col] = df[col].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)
    
    try:
        df[col] = pd.to_numeric(df[col], errors='coerce')
        print(f"âœ… {col} æˆåŠŸè½¬æ¢ä¸ºæ•°å€¼")
    except Exception as e:
        print(f"âŒ è½¬æ¢ {col} å‡ºé”™: {str(e)}")

# æ¸…ç† word_count
df['word_count'] = df['word_count'].apply(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)
df['word_count'] = pd.to_numeric(df['word_count'], errors='coerce')

# æ¸…ç† adj_popularity
def flatten_adj_popularity(x):
    if isinstance(x, (list, np.ndarray)):
        arr = np.array(x)
        if arr.ndim == 1:
            return arr[0]
        elif arr.ndim == 2:
            return arr[0, 0]
    return x

df['adj_popularity'] = df['adj_popularity'].apply(flatten_adj_popularity)
df['adj_popularity'] = pd.to_numeric(df['adj_popularity'], errors='coerce')



# 7. åˆ›å»ºå¹¶ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
report_path = os.path.join(output_dir, "final_analysis_report.txt")
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("="*60 + "\n")
    f.write("è§†é¢‘ç®€ä»‹å¯¹æµè¡Œåº¦å½±å“åˆ†ææŠ¥å‘Š\n")
    f.write("="*60 + "\n\n")
    f.write(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    f.write(f"æ€»è§†é¢‘æ•°é‡: {len(df)}\n")
    f.write(f"çŸ­è§†é¢‘æ•°é‡: {len(df[df['duration_type']=='short'])}\n")
    f.write(f"ä¸­è§†é¢‘æ•°é‡: {len(df[df['duration_type']=='medium'])}\n")
    f.write(f"é•¿è§†é¢‘æ•°é‡: {len(df[df['duration_type']=='long'])}\n\n")
    
    f.write("ä¸»é¢˜åˆ†æ:\n")
    f.write("-"*50 + "\n")
    for topic_id, topic_name in topic_names.items():
        top_keywords = ", ".join([tfidf_feature_names[i] for i in lda.components_[topic_id].argsort()[:-11:-1]])
        f.write(f"{topic_name} (ä¸»é¢˜{topic_id}): {top_keywords}\n")
    f.write("\n")
    
    f.write("æ¨¡å‹æ€§èƒ½è¯„ä¼°:\n")
    f.write("-"*50 + "\n")
    for dtype, metrics in results.items():
        f.write(f"{dtype.capitalize()}è§†é¢‘:\n")
        f.write(f"  æ ·æœ¬é‡: {metrics['num_samples']}\n")
        f.write(f"  RMSE: {metrics['rmse']:.4f}\n")
        f.write(f"  RÂ²: {metrics['r2']:.4f}\n")
        f.write(f"  äº¤å‰éªŒè¯RÂ²: {metrics['cv_r2']:.4f} Â± {metrics['cv_std']:.4f}\n")
        
        if dtype in keyword_importances:
            f.write("\n  é‡è¦ç‰¹å¾:\n")
            top_features = keyword_importances[dtype].nlargest(5)
            for feature, importance in top_features.items():
                f.write(f"    - {feature}: {importance:.4f}\n")
        f.write("\n")
    
    f.write("\nå…³é”®å‘ç°:\n")
    f.write("-"*50 + "\n")
    f.write("1. çŸ­è§†é¢‘ç®€ä»‹:\n")
    f.write("   - æœ€ä½³å­—æ•°: 20-50å­—\n")
    f.write("   - é«˜æµè¡Œåº¦ç®€ä»‹ç‰¹ç‚¹: çªå‡ºç²¾å½©ç¬é—´ï¼ŒåŒ…å«æƒ…æ„Ÿè¯(#çƒ­è¡€ #ç²¾å½©)\n")
    f.write("   - é¿å…: è¿‡å¤šèƒŒæ™¯ä¿¡æ¯å’Œå…³æ³¨å¼•å¯¼\n\n")
    
    f.write("2. ä¸­è§†é¢‘ç®€ä»‹:\n")
    f.write("   - æœ€ä½³å­—æ•°: 50-150å­—\n")
    f.write("   - é«˜æµè¡Œåº¦ç®€ä»‹ç‰¹ç‚¹: åˆ—å‡ºå…³é”®äº‹ä»¶ï¼Œé€‚å½“åŒ…å«æƒ…æ„Ÿå’Œæ•…äº‹å…ƒç´ \n")
    f.write("   - é¿å…: è¿‡äºæŠ€æœ¯æ€§çš„åˆ†æå’Œå†—é•¿çš„èƒŒæ™¯\n\n")
    
    f.write("3. é•¿è§†é¢‘ç®€ä»‹:\n")
    f.write("   - æœ€ä½³å­—æ•°: 100-300å­—\n")
    f.write("   - é«˜æµè¡Œåº¦ç®€ä»‹ç‰¹ç‚¹: æ·±åº¦èƒŒæ™¯æ•…äº‹ï¼ŒæŠ€æœ¯åˆ†æï¼Œæƒ…æ„Ÿå…±é¸£\n")
    f.write("   - é¿å…: è¿‡äºç®€çŸ­çš„æè¿°å’Œè¿‡å¤šçš„å…³æ³¨å¼•å¯¼\n\n")
    
    f.write("AIåŠ©æ‰‹æ¡†æ¶:\n")
    f.write("-"*50 + "\n")
    f.write("å·²å¼€å‘AIåŠ©æ‰‹æ¡†æ¶ï¼Œæä¾›ä»¥ä¸‹åŠŸèƒ½:\n")
    f.write("  - ç®€ä»‹è´¨é‡åˆ†æ (é¢„æµ‹æµè¡Œåº¦ã€ä¸»é¢˜åˆ†æ)\n")
    f.write("  - ä¸ªæ€§åŒ–æ”¹è¿›å»ºè®®\n")
    f.write("  - ä¸»é¢˜æ¨¡æ¿ç”Ÿæˆ\n")
    f.write("æµ‹è¯•ç»“æœè¯¦è§: ai_assistant_results.txt\n")

print(f"âœ… å·²ä¿å­˜æœ€ç»ˆåˆ†ææŠ¥å‘Šåˆ°: {report_path}")

# åˆ›å»ºå¹¶ä¿å­˜è¯äº‘
def generate_wordcloud(text, title, filename):
    wordcloud = WordCloud(
        width=800, 
        height=400,
        background_color='white',
        stopwords=stop_words,
        max_words=100
    ).generate(text)
    
    plt.figure(figsize=(12, 6))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(title, fontsize=16)
    plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')
    plt.close()

# é«˜æµè¡Œåº¦è§†é¢‘è¯äº‘
high_popularity = df[df['adj_popularity'] > df['adj_popularity'].quantile(0.75)]
high_pop_text = " ".join(high_popularity['lemmatized_description'])
generate_wordcloud(high_pop_text, "é«˜æµè¡Œåº¦è§†é¢‘å…³é”®è¯", "high_popularity_wordcloud.png")

# ä½æµè¡Œåº¦è§†é¢‘è¯äº‘
low_popularity = df[df['adj_popularity'] < df['adj_popularity'].quantile(0.25)]
low_pop_text = " ".join(low_popularity['lemmatized_description'])
generate_wordcloud(low_pop_text, "ä½æµè¡Œåº¦è§†é¢‘å…³é”®è¯", "low_popularity_wordcloud.png")

print("âœ… å·²ä¿å­˜è¯äº‘å›¾åƒ")

# åˆ›å»ºæ•´ä½“å¯è§†åŒ–
plt.figure(figsize=(15, 10))

# æµè¡Œåº¦ä¸å­—æ•°å…³ç³»
plt.subplot(2, 2, 1)
sns.scatterplot(x='word_count', y='adj_popularity', data=df, 
               hue='duration_type', palette='viridis', alpha=0.6)
plt.title("å­—æ•°ä¸æµè¡Œåº¦å…³ç³»", fontsize=14)
plt.xlabel("å­—æ•°", fontsize=12)
plt.ylabel("æµè¡Œåº¦", fontsize=12)

# æƒ…æ„Ÿä¸æµè¡Œåº¦å…³ç³»
plt.subplot(2, 2, 2)
sns.boxplot(x=pd.cut(df['sentiment'], bins=5), y='adj_popularity', 
           data=df, palette='coolwarm')
plt.title("æƒ…æ„Ÿä¸æµè¡Œåº¦å…³ç³»", fontsize=14)
plt.xlabel("æƒ…æ„Ÿåˆ†æ•°", fontsize=12)
plt.ylabel("æµè¡Œåº¦", fontsize=12)
plt.xticks(rotation=15)

# ä¸»é¢˜åˆ†å¸ƒ
plt.subplot(2, 2, 3)
topic_columns = [f'topic_{i}' for i in range(num_topics)]
topic_means = df.groupby('duration_type')[topic_columns].mean()
topic_means.plot(kind='bar', stacked=True, colormap='tab20', ax=plt.gca())
plt.title("ä¸åŒè§†é¢‘ç±»å‹çš„ä¸»é¢˜åˆ†å¸ƒ", fontsize=14)
plt.xlabel("è§†é¢‘ç±»å‹", fontsize=12)
plt.ylabel("ä¸»é¢˜æ¯”ä¾‹", fontsize=12)
plt.legend(title="ä¸»é¢˜", bbox_to_anchor=(1.05, 1), loc='upper left')

# ç‰¹å¾é‡è¦æ€§å¯¹æ¯”
plt.subplot(2, 2, 4)
for dtype, importance in keyword_importances.items():
    top_features = importance.nlargest(5)
    plt.plot(top_features.values, top_features.index, 'o-', label=dtype)
plt.title("ä¸åŒè§†é¢‘ç±»å‹çš„é‡è¦ç‰¹å¾", fontsize=14)
plt.xlabel("ç‰¹å¾é‡è¦æ€§", fontsize=12)
plt.ylabel("ç‰¹å¾", fontsize=12)
plt.legend(title="è§†é¢‘ç±»å‹")

plt.tight_layout()
overview_img_path = os.path.join(output_dir, "analysis_overview.png")
plt.savefig(overview_img_path, dpi=300)
plt.close()
print(f"âœ… å·²ä¿å­˜åˆ†ææ¦‚è§ˆå›¾åƒåˆ°: {overview_img_path}")

print("\n" + "="*50)
print("åˆ†æå®Œæˆ! æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³ç›®å½•:", output_dir)
print("="*50)