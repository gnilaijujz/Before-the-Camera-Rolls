import random
import time
import isodate
from googleapiclient.discovery import build
import pandas as pd

# === 1. è®¾ç½®å‚æ•° ===
API_KEYS = [
    'AIzaSyC4FzQ3jVTBKanstgyKdRFj4qp0CDOrIaQ',
    'AIzaSyDMVEJwXu4Rs3amiDIk44QyppzWO_NF0u4',
    'AIzaSyC33-8fcs9okEn8qJIbM3K_oQ4GsJhG75o'
]

# ğŸ‘‡ æ›¿æ¢ä¸ºæ¯äººè´Ÿè´£çš„å­£åº¦æ—¶é—´æ®µ
PUBLISHED_AFTER = '2024-01-01T00:00:00Z'
PUBLISHED_BEFORE = '2024-03-31T23:59:59Z'

SEARCH_QUERY = 'sports'  # åŠ å›å…³é”®è¯ï¼Œæ‰©å¤§æœç´¢ç»“æœ
MAX_RESULTS_TOTAL = 1000  # æ¯å­£åº¦ç›®æ ‡æ•°é‡
RESULTS_PER_PAGE = 50  # æ¯é¡µæœ€å¤§è§†é¢‘æ•°
MAX_PAGES = 50  # æœ€å¤§ç¿»é¡µæ•°

# === 2. è·å– YouTube å®¢æˆ·ç«¯ ===
def get_youtube_client():
    key = random.choice(API_KEYS)
    return build('youtube', 'v3', developerKey=key)

# === 3. æœç´¢è§†é¢‘ID ===
def get_video_ids():
    video_ids = []
    next_page_token = None
    page_count = 0

    while len(video_ids) < MAX_RESULTS_TOTAL and page_count < MAX_PAGES:
        try:
            youtube = get_youtube_client()
            response = youtube.search().list(
                q=SEARCH_QUERY,
                type='video',
                part='id',
                maxResults=RESULTS_PER_PAGE,
                pageToken=next_page_token,
                publishedAfter=PUBLISHED_AFTER,
                publishedBefore=PUBLISHED_BEFORE
            ).execute()

            page_ids = [item['id']['videoId'] for item in response['items'] if item['id']['kind'] == 'youtube#video']
            video_ids.extend(page_ids)

            print(f"ğŸ“„ ç¬¬ {page_count + 1} é¡µï¼Œè·å– {len(page_ids)} æ¡ï¼Œæ€»è®¡ {len(video_ids)} æ¡")

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break

            page_count += 1
            time.sleep(1)
        except Exception as e:
            print(f"Search error: {e}")
            time.sleep(2)

    return video_ids[:MAX_RESULTS_TOTAL]

# === 4. è·å–è§†é¢‘è¯¦ç»†æ•°æ® ===
def get_video_details(video_ids):
    video_data = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        youtube = get_youtube_client()
        try:
            response = youtube.videos().list(
                part='snippet,contentDetails,statistics',
                id=','.join(batch)
            ).execute()

            for item in response['items']:
                vid = item['id']
                snippet = item['snippet']
                stats = item.get('statistics', {})

                title = snippet.get('title', '')
                description = snippet.get('description', '')
                published_at = snippet.get('publishedAt', '')
                tags = snippet.get('tags', [])
                duration = isodate.parse_duration(item['contentDetails']['duration']).total_seconds()
                url = f'https://www.youtube.com/watch?v={vid}'

                # ç»Ÿè®¡æ•°æ®
                view_count = int(stats.get('viewCount', 0))
                like_count = int(stats.get('likeCount', 0))
                #dislike_count = int(stats.get('dislikeCount', 0))  # å·²ä¸å†å…¬å¼€ï¼Œç»“æœä¸º0
                comment_count = int(stats.get('commentCount', 0))
                #share_count = 0  # æ—  API æ¥å£è·å–ï¼Œé»˜è®¤è®¾ä¸º 0

                # è§‚ä¼—ç•™å­˜ç‡ä¸äº’åŠ¨ç‡ (å ä½ç¬¦æ¨¡æ‹Ÿå€¼)
                #retention_rate = round(random.uniform(0.2, 0.9), 2)  # æ¨¡æ‹Ÿï¼š0.2~0.9
                engagement_rate = round((like_count + comment_count) / max(view_count, 1), 4)

                video_data.append({
                    'video_id': vid,
                    'url': url,
                    'duration_seconds': int(duration),
                    'published_at': published_at,
                    'tags': ', '.join(tags),
                    'title': title,
                    'description': description,
                    'view_count': view_count,
                    'like_count': like_count,
                    #'dislike_count': dislike_count,
                    'comment_count': comment_count,
                    #'share_count': share_count,
                    #'retention_rate': retention_rate,
                    'engagement_rate': engagement_rate
                })
        except Exception as e:
            print(f"Details error: {e}")
            time.sleep(2)

    return video_data

# === 5. ä¿å­˜ä¸º CSV ===
def save_to_csv(data, filename):
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"âœ… å·²ä¿å­˜ä¸ºï¼š{filename}")

# === 6. ä¸»ç¨‹åº ===
if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹çˆ¬å–...")
    ids = get_video_ids()
    print(f"ğŸ” è·å–è§†é¢‘æ•°é‡: {len(ids)}")

    info = get_video_details(ids)
    print(f"ğŸ“¦ è·å–è¯¦æƒ…æ•°é‡: {len(info)}")

    save_to_csv(info, 'sports_q1.csv')  # ğŸ‘ˆ æ¯äººæ”¹æˆ q2/q3/q4 å³å¯