import random
import time
import csv
import isodate
from googleapiclient.discovery import build
import pandas as pd

# ğŸ‘‡ æ¯äººæ›¿æ¢ä¸ºè‡ªå·±3ä¸ªAPI KEY
API_KEYS = [
    'AIzaSyC4FzQ3jVTBKanstgyKdRFj4qp0CDOrIaQ',
    'AIzaSyDMVEJwXu4Rs3amiDIk44QyppzWO_NF0u4',
    'AIzaSyC33-8fcs9okEn8qJIbM3K_oQ4GsJhG75o'
]

# ğŸ‘‡ æ¯äººè®¾ç½®è‡ªå·±è´Ÿè´£çš„æ—¶é—´æ®µï¼ˆæ¯å­£åº¦ï¼‰
PUBLISHED_AFTER = '2024-01-01T00:00:00Z'   # ğŸ‘ˆ Q1 ç¤ºä¾‹
PUBLISHED_BEFORE = '2024-03-31T23:59:59Z'

# ğŸ‘‡ æœç´¢å…³é”®è¯åŠåˆ†ç±»
SEARCH_QUERY = 'sports'
VIDEO_CATEGORY_ID = '17'
MAX_PAGES = 10  # æ¯äººæœ€å¤šç¿»å¤šå°‘é¡µï¼ˆæ¯é¡µæœ€å¤š50æ¡ï¼‰

# è·å– YouTube å®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨æ¢ keyï¼‰
def get_youtube_client():
    key = random.choice(API_KEYS)
    print(f"âœ… å½“å‰ä½¿ç”¨ API Key: {key}")
    return build('youtube', 'v3', developerKey=key)

# è·å–è§†é¢‘IDï¼ˆæŒ‰å­£åº¦ + å¤šé¡µï¼‰
def get_video_ids():
    youtube = get_youtube_client()
    video_ids = []
    next_page_token = None

    for page in range(MAX_PAGES):
        try:
            response = youtube.search().list(
                q=SEARCH_QUERY,
                type='video',
                videoCategoryId=VIDEO_CATEGORY_ID,
                part='id',
                maxResults=50,
                pageToken=next_page_token,
                publishedAfter=PUBLISHED_AFTER,
                publishedBefore=PUBLISHED_BEFORE
            ).execute()

            for item in response['items']:
                video_ids.append(item['id']['videoId'])

            next_page_token = response.get('nextPageToken')
            if not next_page_token:
                break
            time.sleep(1)

        except Exception as e:
            print(f"âŒ Error: {e}")
            time.sleep(2)

    return video_ids

# è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
def get_video_info(video_ids):
    all_info = []
    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]
        youtube = get_youtube_client()
        try:
            response = youtube.videos().list(
                part='contentDetails,snippet',
                id=','.join(batch)
            ).execute()

            for item in response['items']:
                vid = item['id']
                title = item['snippet']['title']
                duration = isodate.parse_duration(item['contentDetails']['duration']).total_seconds()
                url = f'https://www.youtube.com/watch?v={vid}'

                all_info.append({
                    'video_id': vid,
                    'title': title,
                    'duration_seconds': int(duration),
                    'url': url,
                    'published_at': item['snippet']['publishedAt']
                })
        except Exception as e:
            print(f"âŒ è§†é¢‘è¯¦æƒ…è·å–å¤±è´¥: {e}")
            time.sleep(2)

    return all_info

# ä¿å­˜ä¸º CSV
def save_to_csv(data, filename):
    df = pd.DataFrame(data)
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"âœ… å·²ä¿å­˜ä¸º: {filename}")

# ä¸»ç¨‹åº
if __name__ == '__main__':
    print("ğŸš€ å¼€å§‹çˆ¬å– ...")
    ids = get_video_ids()
    print(f"ğŸ¯ è·å–è§†é¢‘æ•°é‡ï¼š{len(ids)}")

    info = get_video_info(ids)
    print(f"ğŸ“¦ è·å–å®Œæ•´ä¿¡æ¯æ•°é‡ï¼š{len(info)}")

    # æ¯äººä¿å­˜ä¸åŒæ–‡ä»¶åï¼ˆå¦‚ q1.csvï¼‰
    save_to_csv(info, 'sports_q1.csv')
